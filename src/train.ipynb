{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, f1_score, accuracy_score, \n",
    "    precision_score, recall_score, roc_auc_score\n",
    ")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dataset import MoleculeDataset\n",
    "from model import MoleculeNet\n",
    "\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device selected: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device selected:\", device)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_one_epoch(epoch, model, train_loader, optimizer, loss_fn):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "\n",
    "    for _, batch in enumerate(tqdm(train_loader)):\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch.x.float(),\n",
    "                     batch.edge_attr.float(),\n",
    "                     batch.edge_index,\n",
    "                     batch.batch)\n",
    "        # Calculating the loss and gradients\n",
    "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
    "    return running_loss / step\n",
    "\n",
    "def test(epoch, model, test_loader, loss_fn):\n",
    "    all_preds = []\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for batch in test_loader:\n",
    "        batch.to(device)  \n",
    "        pred = model(batch.x.float(), \n",
    "                        batch.edge_attr.float(),\n",
    "                        batch.edge_index, \n",
    "                        batch.batch) \n",
    "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
    "\n",
    "         # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
    "        all_preds_raw.append(torch.sigmoid(pred).cpu().detach().numpy())\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    print(all_preds_raw[0][:10])\n",
    "    print(all_preds[:10])\n",
    "    print(all_labels[:10])\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"test\")\n",
    "    log_conf_matrix(all_preds, all_labels, epoch)\n",
    "    return running_loss/step\n",
    "\n",
    "def log_conf_matrix(y_pred, y_true, epoch):\n",
    "    # Log confusion matrix as image\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    classes = [\"0\", \"1\"]\n",
    "    df_cfm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    cfm_plot = sns.heatmap(df_cfm, annot=True, cmap='Blues', fmt='g')\n",
    "    cfm_plot.figure.savefig(f'data/images/cm_{epoch}.png')\n",
    "    mlflow.log_artifact(f\"data/images/cm_{epoch}.png\")\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, epoch, type):\n",
    "    print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nF1 score:\", f1_score(y_true, y_pred))\n",
    "    print(\"\\nAccuracy score:\", accuracy_score(y_true, y_pred))\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    print(f\"Precision score:\", precision)\n",
    "    print(f\"Recall score:\", recall)\n",
    "\n",
    "    mlflow.log_metric(key=f\"Precision-{type}\", value=float(precision), step=epoch)\n",
    "    mlflow.log_metric(key=f\"Recall-{type}\", value=float(recall), step=epoch)\n",
    "\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true, y_pred)\n",
    "        print(\"ROC AUC:\", roc)\n",
    "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(roc), step=epoch)\n",
    "    except:\n",
    "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=0.0, step=epoch)\n",
    "        print(\"[Exception] ROC AUC: Not Defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mango import Tuner, scheduler\n",
    "from config import HYPERPARAMETERS, BEST_PARAMETERS, SIGNATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_training(params):\n",
    "    params = params[0]\n",
    "    with mlflow.start_run() as run:\n",
    "        for key in params.keys():\n",
    "            mlflow.log_param(key, params[key])\n",
    "        \n",
    "        print(\"Loading dataset...\")\n",
    "        train_dataset = MoleculeDataset(root=\"../data/\", filename=\"HIV_train.csv\")\n",
    "        test_dataset = MoleculeDataset(root=\"../data/\", filename=\"HIV_test.csv\", test=True)\n",
    "        params[\"model_edge_dim\"] = train_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        print(\"Loading model...\")\n",
    "        model_params = {k: v for k, v in params.items() if k.startswith(\"model_\")}\n",
    "        model = MoleculeNet(feature_size=train_dataset[0].x.shape[1], model_params=model_params)\n",
    "        model = model.to(device)\n",
    "        print(\"Number of parameters:\", count_parameters(model))\n",
    "        mlflow.log_param(\"num_params\", count_parameters(model))\n",
    "\n",
    "        # < 1 increases precision, > 1 recall\n",
    "        weight = torch.tensor([params[\"pos_weight\"]], dtype=torch.float32).to(device)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params[\"learning_rate\"], momentum=params[\"sgd_momentum\"], weight_decay=params[\"weight_decay\"])\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=params[\"scheduler_gamma\"])\n",
    "\n",
    "        # Start training\n",
    "        best_loss = 1000\n",
    "        early_stopping_counter = 0\n",
    "        for epoch in range(300):\n",
    "            if early_stopping_counter <= 10:\n",
    "                # Training\n",
    "                model.train()\n",
    "                loss = train_one_epoch(epoch, model, train_loader, optimizer, loss_fn)\n",
    "                print(f\"Epoch {epoch+1} | Train loss: {loss}\")\n",
    "                mlflow.log_metric(key=\"Train loss\", value=float(loss), step=epoch+1)\n",
    "\n",
    "                # Validation\n",
    "                model.eval()\n",
    "                if epoch % 5 == 0:\n",
    "                    loss = test(epoch, model, test_loader, loss_fn)\n",
    "                    print(f\"Epoch {epoch+1} | Validation loss: {loss}\")\n",
    "                    mlflow.log_metric(key=\"Validation loss\", value=float(loss), step=epoch+1)\n",
    "\n",
    "                    if float(loss) < best_loss:\n",
    "                        best_loss = loss\n",
    "                        mlflow.pytorch.log_model(model, \"model\", signature=SIGNATURE)\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "                \n",
    "                scheduler.step()\n",
    "\n",
    "            else:\n",
    "                print(\"Early stopping due to no improvement.\")\n",
    "                return [best_loss]\n",
    "    \n",
    "    print(\"Finishing training with best validation loss:\", best_loss)\n",
    "    return [best_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/14 23:29:42 INFO mlflow.tracking._tracking_service.client: 🏃 View run grandiose-snake-169 at: http://localhost:5000/#/experiments/0/runs/cb56466dbdfa488b971468df6ec895c4.\n",
      "2024/09/14 23:29:42 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Hyperparameter search: \n",
      "Loading dataset...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m tuner \u001b[38;5;241m=\u001b[39m Tuner(HYPERPARAMETERS, objective\u001b[38;5;241m=\u001b[39mrun_one_training, conf_dict\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/mango/mango/tuner.py:127\u001b[0m, in \u001b[0;36mTuner.minimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaximize_objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/mango/mango/tuner.py:114\u001b[0m, in \u001b[0;36mTuner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_bayesian:\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunBayesianOptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_random:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunRandomOptimizer()\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/mango/mango/tuner.py:176\u001b[0m, in \u001b[0;36mTuner.runBayesianOptimizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrunBayesianOptimizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    174\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m--> 176\u001b[0m     X_list, Y_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_initial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# evaluated hyperparameters are used\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     X_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mconvert_GP_space(X_list)\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/mango/mango/tuner.py:154\u001b[0m, in \u001b[0;36mTuner.run_initial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# getting first few random values\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     X_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mget_random_sample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39minitial_random)\n\u001b[0;32m--> 154\u001b[0m     X_list, Y_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunUserObjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# in case initial random results are invalid try different samples\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     n_tries \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/mango/mango/tuner.py:342\u001b[0m, in \u001b[0;36mTuner.runUserObjective\u001b[0;34m(self, X_next_PS)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrunUserObjective\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_next_PS):\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;66;03m# initially assuming entire X_next_PS is evaluated and returned results are only Y values\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     X_list_evaluated \u001b[38;5;241m=\u001b[39m X_next_PS\n\u001b[0;32m--> 342\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_next_PS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     Y_list_evaluated \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# if result is a tuple, then there is possibility that partial values are evaluated\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m, in \u001b[0;36mrun_one_training\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      5\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_param(key, params[key])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMoleculeDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHIV_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m MoleculeDataset(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHIV_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_edge_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39medge_attr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/src/dataset.py:28\u001b[0m, in \u001b[0;36mMoleculeDataset.__init__\u001b[0;34m(self, root, filename, test, transform, pre_transform)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, filename, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pre_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    root (str): The path to the directory where the dataset is stored.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    filename (str): The name of the `.csv` file inside the root directory.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    pre_transform: Transformations performed before loading the dataset\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest \u001b[38;5;241m=\u001b[39m test\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m filename\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/venv/lib/python3.10/site-packages/torch_geometric/data/dataset.py:115\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/venv/lib/python3.10/site-packages/torch_geometric/data/dataset.py:255\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m osp\u001b[38;5;241m.\u001b[39mexists(f) \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m _repr(\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_filter):\n\u001b[1;32m    249\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `pre_filter` argument differs from the one used in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe pre-processed version of this dataset. If you want to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake use of another pre-fitering technique, pass \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`force_reload=True` explicitly to reload the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_reload \u001b[38;5;129;01mand\u001b[39;00m files_exist(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_paths\u001b[49m):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytest\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/venv/lib/python3.10/site-packages/torch_geometric/data/dataset.py:212\u001b[0m, in \u001b[0;36mDataset.processed_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessed_paths\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The absolute filepaths that must be present in order to skip\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    processing.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_file_names\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# Prevent a common source of error in which `file_names` are not\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# defined as a property.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, Callable):\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/src/dataset.py:35\u001b[0m, in \u001b[0;36mMoleculeDataset.processed_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessed_file_names\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"If these files are found in `raw_dir`, data processing is skipped.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_paths\u001b[49m[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_test_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mindex)]\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/venv/lib/python3.10/site-packages/torch_geometric/data/dataset.py:200\u001b[0m, in \u001b[0;36mDataset.raw_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_paths\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The absolute filepaths that must be present in order to skip\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    downloading.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_file_names\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# Prevent a common source of error in which `file_names` are not\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# defined as a property.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, Callable):\n",
      "File \u001b[0;32m~/Playground/Python/GrapHIV/venv/lib/python3.10/site-packages/torch_geometric/data/dataset.py:64\u001b[0m, in \u001b[0;36mDataset.raw_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_file_names\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m], Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The name of the files in the :obj:`self.raw_dir` folder that must\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    be present in order to skip downloading.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Running Hyperparameter search: \")\n",
    "config = dict()\n",
    "config[\"optimizer\"] = \"Bayesian\"\n",
    "config[\"num_iteration\"] = 100\n",
    "\n",
    "tuner = Tuner(HYPERPARAMETERS, objective=run_one_training, conf_dict=config)\n",
    "results = tuner.minimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
